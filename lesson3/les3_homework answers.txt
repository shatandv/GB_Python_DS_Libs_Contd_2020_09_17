1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?
Ответ:
- macro - среднее арифметическое для метрик для каждого класса. (это используется в Scikit-learn)
Здесь абсолютно не важно количество имеющихся экземпляров каждого класса.
(*Что интересно, есть и другой способ подсчитывать macro-F1, хоть он менее рекомендован, чем среднее арифметическое. Это подсчет macro-F1 через macro-recall и macro-precision. Вот интересная статья на эту тему - https://towardsdatascience.com/a-tale-of-two-macro-f1s-8811ddcf8f04).
- weighted - то же, что и macro, но взвешенное по количеству имеющихся экземпляров для каждого класса.
- micro - смотрим на метрики и TP, TN, FP, FN не поклассово, а в общем. То есть количество экземпляров для каждого класса важно при подсчете.
Пример - берем общее число TP, FP, FN вне зависимости от классов. Подсчитываем на их основе micro-precision и micro-recall. Из этих двух метрик можем посчитать micro-F1.

Лучше использовать:
- macro метрику, если нужно дать классам одинаковый вес, вне зависимости от их превалентности в датасете, если их метрики одинаково важны. Либо же, если хотим дать классу с малой долей экземпляров в датасете бОльший вес. Пример: выявление мошенничества - таких случаев малая доля, но именно их нужно правильно опознать.
- weighted метрику, если нужно дать классам вес согласно их превалентности. Пример: классификация того, на какую полку в магазине стоит поставить предмет - более часто покупаемые предметы лучше ставить на видные полки, поэтому нам важна точность для предметов в зависимости от их частоты в датасете (= частота потребления).
- micro метрику, если нам не важен дисбаланс классов и мы просто хотим оптимизировать общую метрику вне зависимости от классов, либо если нам важнее класс с бОльшей долей в датасете. Пример: классификация клиентов по уровням доходности для компании, когда низкодоходных/убыточных мало - они не так важны для компании.



2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
Ответ:
Все 3 модели основаны на градиентном бустинге.

Особенности.
XGBoost:
- нахождение значений сплитов с помощью сортировочного или гистограммного алгоритма.
- деревья растут поуровнево.
- очень популярный алгоритм, постоянно показывавший себя лучшим во многих ML-задачах и соревнованиях.
- отлично параллелизируется как на нескольких ядрах одного CPU, так и на GPU или распределенной сети компьютеров (многие алгоритмы в имеющихся библиотеках написаны под CPU).
- не умеет сам обрабатывать категориальные признаки, предварительная обработка обязательна.
- умеет сам обрабатывать пропущенные значения.
- имеет встроенную регуляризацию для уменьшения переобучения.
- градиентный спуск осуществляется не просто по градиенту, а по методу Ньютона (https://en.wikipedia.org/wiki/Newton's_method_in_optimization), что позволяет собрать больше информации и более точно вычислить направление и шаг спуска.

LightGBM:
- нахождение значений сплитов с помощью GOSS (gradient-base one-side sampling), что быстрее сортировочного и гистограммного алгоритма.
- EFB (excessive feature bundling) помогает облегчить работу с разреженными данными, уменьшая количество признаков.
- умеет сам обрабатывать категориальные переменные через параметр categorical_feature с помощью специального алгоритма из исследования "On Grouping for Maximum Homogeneity" Уолтера Фишера (http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf).
- деревья растут полистово, а не поуровнево.
- также может использовать не только CPU, но и GPU.
- в среднем работает быстрее остальных алгоритмов градиентного бустинга.

СatBoost:
- нахождение значений сплитов с помощью модифицированного гистограммного алгоритма.
- хорошо работает на дефолтных параметрах.
- умеет сам обрабатывать категориальные переменные с помощью one_hot_max_size и собственного метода кодирования (похожего на кодирование среднем, но с меньшим переобучением). Это специализация CatBoost.
- умеет сам обрабатывать пропущенные значения.
- старается предотвратить переобучение с помощью 
- деревья растут поуровнево, но используются симметричные деревья, они на каждом уровне делятся по одному и тому же сплиту того же признака на всех сплитах уровня.
- также может использовать не только CPU, но и GPU.
- при правильно настроенных параметрах для категориальных переменных может работать лучше других алгоритмов градиентного бустинга на датасетах с значительным числом категориальных признаков.
- очень наглядная интерактивная визуализация.