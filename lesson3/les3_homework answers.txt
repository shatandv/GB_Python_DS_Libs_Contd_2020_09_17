1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?
Ответ:
- macro - среднее арифметическое для метрик для каждого класса. (это используется в Scikit-learn)
Здесь абсолютно не важно количество имеющихся экземпляров каждого класса.
(*Что интересно, есть и другой способ подсчитывать macro-F1, хоть он менее рекомендован, чем среднее арифметическое. Это подсчет macro-F1 через macro-recall и macro-precision. Вот интересная статья на эту тему - https://towardsdatascience.com/a-tale-of-two-macro-f1s-8811ddcf8f04).
- weighted - то же, что и macro, но взвешенное по количеству имеющихся экземпляров для каждого класса.
- micro - смотрим на метрики и TP, TN, FP, FN не поклассово, а в общем. То есть количество экземпляров для каждого класса важно при подсчете.
Пример - берем общее число TP, FP, FN вне зависимости от классов. Подсчитываем на их основе micro-precision и micro-recall. Из этих двух метрик можем посчитать micro-F1.

Лучше использовать:
- macro метрику, если нужно дать классам одинаковый вес, вне зависимости от их превалентности в датасете, если их метрики одинаково важны. Либо же, если хотим дать классу с малой долей экземпляров в датасете бОльший вес. Пример: выявление мошенничества - таких случаев малая доля, но именно их нужно правильно опознать.
- weighted метрику, если нужно дать классам вес согласно их превалентности. Пример: классификация того, на какую полку в магазине стоит поставить предмет - более часто покупаемые предметы лучше ставить на видные полки, поэтому нам важна точность для предметов в зависимости от их частоты в датасете (= частота потребления).
- micro метрику, если нам не важен дисбаланс классов и мы просто хотим оптимизировать общую метрику вне зависимости от классов, либо если нам важнее класс с бОльшей долей в датасете. Пример: классификация клиентов по уровням доходности для компании, когда низкодоходных/убыточных мало - они не так важны для компании.



2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?
Ответ:
Все 3 модели основаны на градиентном бустинге.

Особенности.
XGBoost:
- нахождение значений сплитов с помощью сортировочного или гистограммного алгоритма,
- 
LightGBM:
- нахождение значений сплитов с помощью GOSS (gradient-base one-side sampling), что быстрее сортировочного и гистограммного алгоритма,
- 
СatBoost:
- нахождение значений сплитов с помощью ???