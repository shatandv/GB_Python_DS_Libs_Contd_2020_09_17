Курсовая:
- Ноутбук - https://github.com/shatandv/GB_Python_DS_Libs_Contd_2020_09_17/pull/1
- Ник на Kaggle - Andrey Shataev


1. Расскажите, как работает регуляризация в решающих деревьях, какие параметры мы штрафуем в данных алгоритмах?
Ответ:
Регуляризацию деревьев можно проводить с помощью следующих параметров:
- ограничение максимальной глубины дерева (меньше глубина - меньше переобучение),
- обрезание ветвей уже обученного глубокого дерева, добавление информации которых ниже порогового значения,
- ограничение для сплита - минимальное добавление информации при сплите, кол-во минимальных наблюдений должно быть в каждом листе, кол-во минимальных наблюдений в узле дерева для сплита, и т.д.
- ансамбли - в основном бэггинг (Random Forest), но также и градиентный бустинг с регуляризацией - XGBoost, CatBoost, LightGBM.


2. По какому принципу рассчитывается "важность признака (feature_importance)" в ансамблях деревьев?
Ответ:
1 способ - permutation importance.
По сути для оценки важности каждого признака происходит следующее:
- Значения признака в его столбце перемешиваются, тогда как остальные не трогают. Это разрушает связь признака с таргетом.
- Оценивается то, насколько упала accuracy предсказания. Если она упала сильно - признак более важный.
Так делается для каждой переменной, что позволяет понять важность всех признаков в работе модели.

2 способ - с помощью оценки общего добавления информации/энтропии/MSE.
Этот способ применяется в реализации Random Forest в библиотеке Sklearn.
Идея основывается на том, что чем лучше сплит по признаку помогает функции оптимизации, тем он важнее.
Для каждого признака смотрится его вклад в улучшение функции оптимизации при сплите, относительно всех признаков - это и будет важность признака для 1 дерева.
Так как Random Forest - бэггинг, то потом важность признака усредняется по всем деревьям - это и есть конечная важность признака во всем ансамбле.

Источники (есть математика для описанных выше способов, довольно интересно посмотреть):
https://mlcourse.ai/articles/topic5-part3-feature-importance/
Auret, L., & Aldrich, C. (2011). Empirical comparison of tree ensemble variable importance measures. Chemometrics and Intelligent Laboratory Systems, 105(2), 157–170. doi:10.1016/j.chemolab.2010.12.004 
